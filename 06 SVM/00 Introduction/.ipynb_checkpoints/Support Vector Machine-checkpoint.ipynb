{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)\n",
    "\n",
    "Invented in [1963](https://en.wikipedia.org/wiki/Support_vector_machine#History) by [Vladimir N. Vapnik](https://en.wikipedia.org/wiki/Vladimir_Vapnik) and Alexey Ya. Chervonenkis while working at AT&T Bell Labs. Vladimir N. Vapnik joined Facebook AI Research in Nov 2014.\n",
    "\n",
    "In 1992, Bernhard E. Boser, Isabelle M. Guyon and Vladimir N. Vapnik suggested a way to create nonlinear classifiers by applying the kernel trick to maximum-margin hyperplanes.\n",
    "\n",
    "The current standard incarnation (soft margin) was proposed by Corinna Cortes and Vapnik in 1993 and published in 1995.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "1. [Support Vector Machine in Javascript Demo by Karpathy](http://cs.stanford.edu/people/karpathy/svmjs/demo/)\n",
    "\n",
    "2. [SVM](http://www.svms.org/tutorials/)\n",
    "\n",
    "3. [Statsoft](http://www.statsoft.com/Textbook/Support-Vector-Machines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Learning Outcomes:\n",
    "\n",
    "* Introduction\n",
    "* Linear SVM Classification\n",
    "* Polynomial Kernal\n",
    "* Radial Basis Function /  Gaussian Kernel\n",
    "* Support Vector Regression\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "Supervised learning methods used for classification, regression and outliers detection.\n",
    "\n",
    "Let's assume we have two classes here - black and purple. In classification, we are interested in the best way to separate the two classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img\\SVM 1.png\" height=50% width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can separate them with a line such as the example below. You can consider this as an example of how logistic regression would segregate the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img\\SVM 2.png\" height=50% width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are infinite lines (in 2-dimensional space) or hyperplanes (in 3-dimensional space) that can be used to separate the two classes as the example below illustrates. \n",
    "\n",
    "**The term hyperplane essentially means it is a subspace of one dimension less than its ambient space.** If a space is 3-dimensional then its hyperplanes are the 2-dimensional planes, while if the space is 2-dimensional, its hyperplanes are the 1-dimensional lines. ~ [Wikipedia](https://en.wikipedia.org/wiki/Hyperplane)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img\\SVM 3.png\" height=50% width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SVM, the **separating line**, the solid brown line, is the line that allows for largest margin between the two classes. \n",
    "\n",
    "SVM would place the separating line in the middle of the margin, also called maximum margin. SVM will optimise and locate the hyperplane that maximises the margin of the two classes. and at last it drew the seperating line in the middle of margin. therefor it is the best line to seprate the support vectores.\n",
    "\n",
    "The samples(dots) that are closest to the hyperplane are called **support vectors**, circled in red. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img\\SVM 4.png\" height=50% width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 2. Linear SVM Classification\n",
    "\n",
    "\n",
    "* Support Vectors\n",
    "\n",
    "* Separate with a straight line (linearly separable)\n",
    "\n",
    "* Margin\n",
    "\n",
    "  * Hard margin classification\n",
    "      * Strictly based on those that are at the margin between the two classes\n",
    "      * However, this is sensitive to outliers (cons)[similar to normal linear regression]\n",
    "      \n",
    "  * Soft margin classification\n",
    "      * Widen the margin and allows for violation (violation means allowing the margin lines to pass some of support vectores)\n",
    "      * With Python Scikit-Learn, you control the width of the margin\n",
    "      * Control with `C` hyperparameter\n",
    "        * smaller `C` leads to a wider street (the distance between margin lines of two different class samples) but more margin violations\n",
    "        * High `C` - fewer margin violations but ends up with a smaller margin\n",
    "\n",
    "\n",
    "\n",
    "**Note:**\n",
    "\n",
    "* SVM are very sensitive to feature scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sns.load_dataset('iris')\n",
    "# I used seaborn library to load the Iris dataset becuz the column names were pre-embeded in the dataset itself\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# how many unique features does we have in Iris dataet?\n",
    "df.species.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using our desired features to build the SVM model\n",
    "col = ['petal_length', 'petal_width', 'species']\n",
    "# and chunking it...\n",
    "df.loc[:, col].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting some features for our independent features (e.g X values)\n",
    "col = ['petal_length', 'petal_width']\n",
    "X = df.loc[:, col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# converting the species type (target feature to predict) to numbers, for ease of access\n",
    "species_to_num = {'setosa': 0,\n",
    "                  'versicolor': 1,\n",
    "                  'virginica': 2}\n",
    "df['tmp'] = df['species'].map(species_to_num)\n",
    "# assigning it to \"y\"\n",
    "y = df['tmp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentations on each:\n",
    "\n",
    "* [LinearSVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC)\n",
    "\n",
    "  Similar to SVC with parameter kernel=’linear’, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.\n",
    "  \n",
    "  \n",
    "  \n",
    "* [SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)\n",
    "\n",
    "  C-Support Vector Classification.\n",
    "  \n",
    "  The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 0.001\n",
    "# kernel alternatives can be \"linear\" or \"rbf\"\n",
    "clf = svm.SVC(kernel='linear', C=C)\n",
    "#clf = svm.LinearSVC(C=C, loss='hinge')\n",
    "#clf = svm.SVC(kernel='poly', degree=3, C=C)\n",
    "#clf = svm.SVC(kernel='rbf', gamma=0.7, C=C)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict([[6, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so it is virginica then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ploting the linear SVM classifier with Iris samples...\n",
    "Xv = X.values.reshape(-1,1)\n",
    "h = 0.02\n",
    "x_min, x_max = Xv.min(), Xv.max() + 1\n",
    "y_min, y_max = y.min(), y.max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "z = z.reshape(xx.shape)\n",
    "fig = plt.figure(figsize=(16,10))\n",
    "ax = plt.contourf(xx, yy, z, cmap = 'afmhot', alpha=0.3);\n",
    "plt.scatter(X.values[:, 0], X.values[:, 1], c=y, s=80, \n",
    "            alpha=0.9, edgecolors='g');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "z = z.reshape(xx.shape)\n",
    "fig = plt.figure(figsize=(16,10))\n",
    "ax = plt.contourf(xx, yy, z, cmap = 'afmhot', alpha=0.3);\n",
    "plt.scatter(X.values[:, 0], X.values[:, 1], c=y, s=80, \n",
    "            alpha=0.9, edgecolors='g');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "better ploting of Iris dataset using \"orange\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img\\iris - petal length and width.png' width=60%, height=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading and feature selection and blah blah blah...\n",
    "df = sns.load_dataset('iris')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "col = ['petal_length', 'petal_width']\n",
    "X = df.loc[:, col]\n",
    "species_to_num = {'setosa': 0,\n",
    "                  'versicolor': 1,\n",
    "                  'virginica': 2}\n",
    "df['tmp'] = df['species'].map(species_to_num)\n",
    "y = df['tmp']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    train_size=0.8, \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# as the name says, scaling the features...\n",
    "sc_x = StandardScaler()\n",
    "X_std_train = sc_x.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running with linear kernel\n",
    "C = 1.0\n",
    "clf = svm.SVC(kernel='linear', C=C)\n",
    "clf.fit(X_std_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation within Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = cross_val_score(clf, X_std_train, y_train, cv=10, scoring='accuracy')\n",
    "print(\"Average Accuracy: \\t {0:.4f}\".format(np.mean(res)))\n",
    "print(\"Accuracy SD: \\t\\t {0:.4f}\".format(np.std(res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_pred = cross_val_predict(clf, X_std_train, y_train, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision Score: \\t {0:.4f}\".format(precision_score(y_train, \n",
    "                                                           y_train_pred, \n",
    "                                                           average='weighted')))\n",
    "print(\"Recall Score: \\t\\t {0:.4f}\".format(recall_score(y_train,\n",
    "                                                     y_train_pred, \n",
    "                                                     average='weighted')))\n",
    "print(\"F1 Score: \\t\\t {0:.4f}\".format(f1_score(y_train,\n",
    "                                             y_train_pred, \n",
    "                                             average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation within Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = cross_val_predict(clf, sc_x.transform(X_test), y_test, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision Score: \\t {0:.4f}\".format(precision_score(y_test, \n",
    "                                                           y_test_pred, \n",
    "                                                           average='weighted')))\n",
    "print(\"Recall Score: \\t\\t {0:.4f}\".format(recall_score(y_test,\n",
    "                                                     y_test_pred, \n",
    "                                                     average='weighted')))\n",
    "print(\"F1 Score: \\t\\t {0:.4f}\".format(f1_score(y_test,\n",
    "                                             y_test_pred, \n",
    "                                             average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 3. Polynomial Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src='img\\polynomial.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# igniting it with polynomial kernel...\n",
    "C = 1.0\n",
    "clf = svm.SVC(kernel='poly', degree=3, C=C)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xv = X.values.reshape(-1,1)\n",
    "h = 0.02\n",
    "x_min, x_max = Xv.min(), Xv.max() + 1\n",
    "y_min, y_max = y.min(), y.max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "z = z.reshape(xx.shape)\n",
    "fig = plt.figure(figsize=(16,10))\n",
    "# ploting a contour for dear poly SVM bclassifier \n",
    "ax = plt.contourf(xx, yy, z, cmap = 'afmhot', alpha=0.3);\n",
    "plt.scatter(X.values[:, 0], X.values[:, 1], c=y, s=80, \n",
    "            alpha=0.5, edgecolors='g');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "acually we see, the model has about 5 number of missclassifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial SVM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boring initialize stuff...\n",
    "df = sns.load_dataset('iris')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "col = ['petal_length', 'petal_width']\n",
    "X = df.loc[:, col]\n",
    "species_to_num = {'setosa': 0,\n",
    "                  'versicolor': 1,\n",
    "                  'virginica': 2}\n",
    "df['tmp'] = df['species'].map(species_to_num)\n",
    "y = df['tmp']\n",
    "X_train, X_std_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        train_size=0.8, \n",
    "                                                        random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Okay, you got it\n",
    "sc_x = StandardScaler()\n",
    "X_std_train = sc_x.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 1.0\n",
    "clf = svm.SVC(kernel='poly', degree=3, C=C)\n",
    "clf.fit(X_std_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation within Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = cross_val_score(clf, X_std_train, y_train, cv=10, scoring='accuracy')\n",
    "print(\"Average Accuracy: \\t {0:.4f}\".format(np.mean(res)))\n",
    "print(\"Accuracy SD: \\t\\t {0:.4f}\".format(np.std(res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_pred = cross_val_predict(clf, X_std_train, y_train, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision Score: \\t {0:.4f}\".format(precision_score(y_train, \n",
    "                                                           y_train_pred, \n",
    "                                                           average='weighted')))\n",
    "print(\"Recall Score: \\t\\t {0:.4f}\".format(recall_score(y_train,\n",
    "                                                     y_train_pred, \n",
    "                                                     average='weighted')))\n",
    "print(\"F1 Score: \\t\\t {0:.4f}\".format(f1_score(y_train,\n",
    "                                             y_train_pred, \n",
    "                                             average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation within Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = cross_val_predict(clf, sc_x.transform(X_test), y_test, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision Score: \\t {0:.4f}\".format(precision_score(y_test, \n",
    "                                                           y_test_pred, \n",
    "                                                           average='weighted')))\n",
    "print(\"Recall Score: \\t\\t {0:.4f}\".format(recall_score(y_test,\n",
    "                                                     y_test_pred, \n",
    "                                                     average='weighted')))\n",
    "print(\"F1 Score: \\t\\t {0:.4f}\".format(f1_score(y_test,\n",
    "                                             y_test_pred, \n",
    "                                             average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 4. Gaussian Radial Basis Function (rbf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel function can be any of the following:\n",
    "\n",
    "* linear: $\\langle x, x'\\rangle$.\n",
    "\n",
    "\n",
    "* polynomial: $(\\gamma \\langle x, x'\\rangle + r)^d$. \n",
    "\n",
    "  $d$ is specified by keyword `degree`\n",
    "  \n",
    "  $r$ by `coef0`.\n",
    "\n",
    "\n",
    "* rbf: $\\exp(-\\gamma \\|x-x'\\|^2)$. \n",
    "\n",
    "  $\\gamma$ is specified by keyword `gamma` must be greater than 0.\n",
    "\n",
    "\n",
    "* sigmoid $(\\tanh(\\gamma \\langle x,x'\\rangle + r))$\n",
    "\n",
    "  where $r$ is specified by `coef0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sns.load_dataset('iris')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "col = ['petal_length', 'petal_width']\n",
    "X = df.loc[:, col]\n",
    "species_to_num = {'setosa': 0,\n",
    "                  'versicolor': 1,\n",
    "                  'virginica': 2}\n",
    "df['tmp'] = df['species'].map(species_to_num)\n",
    "y = df['tmp']\n",
    "X_train, X_std_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        train_size=0.8, \n",
    "                                                        random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc_x = StandardScaler()\n",
    "X_std_train = sc_x.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 1.0\n",
    "clf = svm.SVC(kernel='rbf', gamma=0.7, C=C)\n",
    "clf.fit(X_std_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation within Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = cross_val_score(clf, X_std_train, y_train, cv=10, scoring='accuracy')\n",
    "print(\"Average Accuracy: \\t {0:.4f}\".format(np.mean(res)))\n",
    "print(\"Accuracy SD: \\t\\t {0:.4f}\".format(np.std(res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_pred = cross_val_predict(clf, X_std_train, y_train, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision Score: \\t {0:.4f}\".format(precision_score(y_train, \n",
    "                                                           y_train_pred, \n",
    "                                                           average='weighted')))\n",
    "print(\"Recall Score: \\t\\t {0:.4f}\".format(recall_score(y_train,\n",
    "                                                     y_train_pred, \n",
    "                                                     average='weighted')))\n",
    "print(\"F1 Score: \\t\\t {0:.4f}\".format(f1_score(y_train,\n",
    "                                             y_train_pred, \n",
    "                                             average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main idea here is all about creating a permutation of all hyperparameter values inorder to find the best ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can improve this by scaling the features (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('clf', svm.SVC(kernel='rbf', C=1, gamma=0.1))]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'clf__C':(0.1, 0.5, 1, 2, 5, 10, 20), \n",
    "          'clf__gamma':(0.001, 0.01, 0.1, 0.25, 0.5, 0.75, 1)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting minus one to number of job in order to use all of precessor potential.\n",
    "# calling the grid search cross validate module\n",
    "svm_grid_rbf = GridSearchCV(pipeline, params, n_jobs=-1,\n",
    "                            cv=3, verbose=1, scoring='accuracy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and fit them all\n",
    "svm_grid_rbf.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best findings U have?!\n",
    "svm_grid_rbf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calling \"best_estimator_.get_params()\" to achieve the best values for our parameters\n",
    "best = svm_grid_rbf.best_estimator_.get_params() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and what are they?!\n",
    "for k in sorted(params.keys()): \n",
    "    print('\\t{0}: \\t {1:.2f}'.format(k, best[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# applying the model with 'SVM grid search' approach to the test dataset\n",
    "y_test_pred = svm_grid_rbf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_test_pred)\n",
    "# and we have no miss classification !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision Score: \\t {0:.4f}\".format(precision_score(y_test, \n",
    "                                                           y_test_pred, \n",
    "                                                           average='weighted')))\n",
    "print(\"Recall Score: \\t\\t {0:.4f}\".format(recall_score(y_test,\n",
    "                                                     y_test_pred, \n",
    "                                                     average='weighted')))\n",
    "print(\"F1 Score: \\t\\t {0:.4f}\".format(f1_score(y_test,\n",
    "                                             y_test_pred, \n",
    "                                             average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 5. Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "% matplotlib inline\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oh, boston prices again !\n",
    "from sklearn.datasets import load_boston\n",
    "boston_data = load_boston()\n",
    "# creatinga data frame and adding the column names...\n",
    "df = pd.DataFrame(boston_data.data, columns=boston_data.feature_names)\n",
    "# see what we got?!\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = boston_data.target\n",
    "# only using one feature to continue...\n",
    "X = df[['LSTAT']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating and fitting\n",
    "svr = SVR()\n",
    "svr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sorting the indexes \n",
    "# it really helps most of the times\n",
    "sort_idx = X.flatten().argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# draw them all !\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(X[sort_idx], y[sort_idx])\n",
    "plt.plot(X[sort_idx], svr.predict(X[sort_idx]), color='k')\n",
    "\n",
    "plt.xlabel('LSTAT')\n",
    "plt.ylabel('MEDV');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what we see above is a classic overfitting problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So we are gonna examine different kernels for **Support Vector Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the linear kernel for Support Vector Regression (with quite default parameters)\n",
    "svr = SVR(kernel='linear', gamma='auto')\n",
    "# and fitting the model\n",
    "svr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_pred = svr.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = svr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "printing the MSE and the R^2 values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MSE train: {0:.4f}, test: {1:.4f}\".\\\n",
    "      format(mean_squared_error(y_train, y_train_pred), \n",
    "             mean_squared_error(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R^2 train: {0:.4f}, test: {1:.4f}\".\\\n",
    "      format(r2_score(y_train, y_train_pred),\n",
    "             r2_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Polynomial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVR(kernel='poly', C=1e3, degree=2)\n",
    "svr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_pred = svr.predict(X_train)\n",
    "y_test_pred = svr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MSE train: {0:.4f}, test: {1:.4f}\".\\\n",
    "      format(mean_squared_error(y_train, y_train_pred), \n",
    "             mean_squared_error(y_test, y_test_pred)))\n",
    "print(\"R^2 train: {0:.4f}, test: {1:.4f}\".\\\n",
    "      format(r2_score(y_train, y_train_pred),\n",
    "             r2_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**rbf Kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "svr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_pred = svr.predict(X_train)\n",
    "y_test_pred = svr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MSE train: {0:.4f}, test: {1:.4f}\".\\\n",
    "      format(mean_squared_error(y_train, y_train_pred), \n",
    "             mean_squared_error(y_test, y_test_pred)))\n",
    "print(\"R^2 train: {0:.4f}, test: {1:.4f}\".\\\n",
    "      format(r2_score(y_train, y_train_pred),\n",
    "             r2_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 6. Advantages and Disadvantages\n",
    "\n",
    "\n",
    "\n",
    "The **advantages** of support vector machines are:\n",
    "* Effective in high dimensional spaces.\n",
    "* Uses only a subset of training points (support vectors) in the decision function.\n",
    "* So many different Kernel functions can be specified for the decision function.\n",
    "    * Linear\n",
    "    * Polynomial\n",
    "    * RBF\n",
    "    * Sigmoid\n",
    "    * Custom\n",
    "\n",
    "\n",
    "The **disadvantages** of support vector machines include:\n",
    "* Beware of overfitting when num_features > num_samples.\n",
    "* Choice of Kernal and Regularization can have a large impact on performance\n",
    "* No probability estimates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Summary\n",
    "\n",
    "\n",
    "| Class |  Out-of-core support | Kernel Trick |\n",
    "| :- |  :- | :- | :- |\n",
    "| `SGDClassifier` |  Yes | No |\n",
    "| `LinearSVC` |  No | No |\n",
    "| `SVC` |  No | Yes |\n",
    "\n",
    "**Note:** All require features scaling\n",
    "\n",
    "Support Vector Machine algorithms are not scale invariant, so it is highly recommended to scale your data. For example, scale each attribute on the input vector X to [0,1] or [-1,+1], or standardize it to have mean 0 and variance 1. Note that the same scaling must be applied to the test vector to obtain meaningful results. See section Preprocessing data for more details on scaling and normalization. ~ [scikit-learn documentation](http://scikit-learn.org/stable/modules/svm.html#svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where to From Here\n",
    "\n",
    "* [LIBSVM](http://www.csie.ntu.edu.tw/~cjlin/libsvm/)\n",
    "* [LIBLINEAR](http://www.csie.ntu.edu.tw/~cjlin/liblinear/)\n",
    "* [Hands-On Machine Learning with Scikit-Learn and TensorFlow](https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch05.html#svm_chapter)\n",
    "* [Python Data Science Handbook](https://www.safaribooksonline.com/library/view/python-data-science/9781491912126/ch05.html#in-depth-support-vector-machines)\n",
    "* [Python Machine Learning, 2E](https://www.safaribooksonline.com/library/view/python-machine-learning/9781787125933/ch03s04.html)\n",
    "* [Statistics for Machine Learning](https://www.safaribooksonline.com/library/view/statistics-for-machine/9781788295758/f2c95085-6676-41c6-876e-ab6802666ea2.xhtml)\n",
    "* [ConvNetJS](http://cs.stanford.edu/people/karpathy/convnetjs/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
