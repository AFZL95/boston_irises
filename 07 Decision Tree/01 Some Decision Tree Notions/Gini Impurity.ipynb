{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Decision Tree Learning\n",
    "\n",
    "* [ID3](https://en.wikipedia.org/wiki/ID3_algorithm) (Iterative Dichotomiser 3)\n",
    "* [C4.5](https://en.wikipedia.org/wiki/C4.5_algorithm) (successor of ID3)\n",
    "* CART (Classification And Regression Tree)\n",
    "* [CHAID](http://www.statisticssolutions.com/non-parametric-analysis-chaid/) (Chi-squared Automatic Interaction Detector). by [Gordon Kass](https://en.wikipedia.org/wiki/Chi-square_automatic_interaction_detection). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree algorithms: ID3, C4.5, C5.0 and CART\n",
    "\n",
    "\n",
    "* ID3 (Iterative Dichotomiser 3) was developed in 1986 by Ross Quinlan. The algorithm creates a multiway tree, finding for each node (i.e. in a greedy manner) the categorical feature that will yield the largest information gain for categorical targets. Trees are grown to their maximum size and then a pruning step is usually applied to improve the ability of the tree to generalise to unseen data.\n",
    "\n",
    "\n",
    "* C4.5 is the successor to ID3 and removed the restriction that features must be categorical by dynamically defining a discrete attribute (based on numerical variables) that partitions the continuous attribute value into a discrete set of intervals. C4.5 converts the trained trees (i.e. the output of the ID3 algorithm) into sets of if-then rules. These accuracy of each rule is then evaluated to determine the order in which they should be applied. Pruning is done by removing a rule’s precondition if the accuracy of the rule improves without it.\n",
    "\n",
    "\n",
    "* C5.0 is Quinlan’s latest version release under a proprietary license. It uses less memory and builds smaller rulesets than C4.5 while being more accurate.\n",
    "\n",
    "\n",
    "* CART (Classification and Regression Trees) is very similar to C4.5, but it differs in that it supports numerical target variables (regression) and does not compute rule sets. CART constructs binary trees using the feature and threshold that yield the largest information gain at each node.\n",
    "\n",
    "\n",
    "* CHAID (Chi-squared Automatic Interaction Detector). by Gordon Kass. Performs multi-level splits when computing classification trees. Non-parametric. Does not require the data to be normally distributed. \n",
    "\n",
    "scikit-learn uses an optimised version of the CART algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gini Impurity\n",
    "\n",
    "scikit-learn default\n",
    "\n",
    "[Gini Impurity](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity)\n",
    "\n",
    "A measure of purity / variability of categorical data\n",
    "\n",
    "As a side note on the difference between [Gini Impurity and Gini Coefficient](https://datascience.stackexchange.com/questions/1095/gini-coefficient-vs-gini-impurity-decision-trees)\n",
    "\n",
    "* No, despite their names they are not equivalent or even that similar.\n",
    "* **Gini impurity** is a measure of misclassification, which applies in a multiclass classifier context.\n",
    "* **Gini coefficient** applies to binary classification and requires a classifier that can in some way rank examples according to the likelihood of being in a positive class.\n",
    "* Both could be applied in some cases, but they are different measures for different things. Impurity is what is commonly used in decision trees.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Developed by [Corrado Gini](https://en.wikipedia.org/wiki/Corrado_Gini) in 1912\n",
    "\n",
    "Key Points:\n",
    "* A pure node (homogeneous contents or samples with the same class) will have a Gini coefficient of zero\n",
    "* As the variation increases (heterogeneneous classes or increase diversity), Gini coefficient increases and approaches 1.\n",
    "\n",
    "$$Gini=1-\\sum^r_j p^2_j$$\n",
    "\n",
    "$p$ is the probability (often based on the frequency table)\n",
    "\n",
    "for example p=1 means the infinite class (category) diversity [which is so bad !]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img\\gini_imp.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Entropy\n",
    "\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Entropy_(information_theory)\n",
    "\n",
    "The entropy can explicitly be written as\n",
    "\n",
    "$${\\displaystyle \\mathrm {H} (X)=\\sum _{i=1}^{n}{\\mathrm {P} (x_{i})\\,\\mathrm {I} (x_{i})}=-\\sum _{i=1}^{n}{\\mathrm {P} (x_{i})\\log _{b}\\mathrm {P} (x_{i})},}$$\n",
    "\n",
    "where `b` is the base of the logarithm used. Common values of `b` are 2, Euler's number `e`, and 10\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Which should I use?\n",
    "\n",
    "[Sebastian Raschka](https://sebastianraschka.com/faq/docs/decision-tree-binary.html)\n",
    "\n",
    "* They tend to generate similar tree\n",
    "* Gini tends to be faster to compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(p):\n",
    "    return (p)*(1 - (p)) + (1 - p)*(1 - (1-p))\n",
    "\n",
    "def entropy(p):\n",
    "    return - p*np.log2(p) - (1 - p)*np.log2((1 - p))\n",
    "\n",
    "def error(p):\n",
    "    return 1 - np.max([p, 1 - p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0.0, 1.0, 0.01)\n",
    "\n",
    "ent = [entropy(p) if p != 0 else None for p in x]\n",
    "\n",
    "sc_ent = [e*0.5 if e else None for e in ent]\n",
    "err = [error(i) for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = plt.subplot(111)\n",
    "for i, lab, ls, c, in zip([ent, sc_ent, gini(x), err], \n",
    "                   ['Entropy', 'Entropy (scaled)', \n",
    "                   'Gini Impurity', \n",
    "                   'Misclassification Error'],\n",
    "                   ['-', '-', '--', '-.'],\n",
    "                   ['black', 'lightgray',\n",
    "                      'red', 'green', 'cyan']):\n",
    "     line = ax.plot(x, i, label=lab, \n",
    "                    linestyle=ls, lw=2, color=c)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15),\n",
    "           ncol=3, fancybox=True, shadow=False)\n",
    "ax.axhline(y=0.5, linewidth=1, color='k', linestyle='--')\n",
    "ax.axhline(y=1.0, linewidth=1, color='k', linestyle='--')\n",
    "plt.ylim([0, 1.1])\n",
    "plt.xlabel('p(i=1)')\n",
    "plt.ylabel('Impurity Index')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
