{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is the go-to linear classification algorithm for two-class problems (e.g binary classification). It is easy to implement, easy to understand and gets great results on a wide variety of problems, even when the expectations the method has for your data are violated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources:\n",
    "\n",
    "[Logistic Regression Tutorial for Machine Learning](http://machinelearningmastery.com/logistic-regression-tutorial-for-machine-learning/)\n",
    "\n",
    "[Logistic Regression for Machine Learning](http://machinelearningmastery.com/logistic-regression-for-machine-learning/)\n",
    "\n",
    "[How To Implement Logistic Regression With Stochastic Gradient Descent From Scratch With Python](http://machinelearningmastery.com/implement-logistic-regression-stochastic-gradient-descent-scratch-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "#### Logistic Regression\n",
    "\n",
    "Logistic regression is named for the function used at the core of the method, the [logistic function](https://en.wikipedia.org/wiki/Logistic_function).\n",
    "\n",
    "The logistic function, also called the **Sigmoid function** was developed by statisticians to describe properties of population growth in ecology, rising quickly and maxing out at the carrying capacity of the environment. Itâ€™s an S-shaped curve that can take any real-valued number and map it into a value between 0 and 1, but never exactly at those limits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "$e$ is the base of the natural logarithms and $x$ is value that you want to transform via the logistic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting the sigmoid function\n",
    "x = np.linspace(-6, 6, num = 100)\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.plot(x, 1 / (1 + np.exp(-x))); # Sigmoid Function\n",
    "plt.title(\"Sigmoid Function\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression equation has a very simiar representation like linear regression. The difference is that the output value being modelled is binary in nature.\n",
    "\n",
    "$$\\hat{y}=\\frac{e^{\\beta_0+\\beta_1x_1}}{1+\\beta_0+\\beta_1x_1}$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\\hat{y}=\\frac{1.0}{1.0+e^{-\\beta_0-\\beta_1x_1}}$$\n",
    "\n",
    "$\\beta_0$ is the intecept term\n",
    "\n",
    "$\\beta_1$ is the coefficient for $x_1$\n",
    "\n",
    "$\\hat{y}$ is the predicted output with real value between 0 and 1. To convert this to binary output of 0 or 1, this would either need to be rounded to an integer value or a cutoff point be provided to specify the class segregation point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is our dataset !\n",
    "# when we see the first values of dataset, we achieve the boolean result (0 & 1)\n",
    "dataset = [[-2.0011, 0],\n",
    "           [-1.4654, 0],\n",
    "           [0.0965, 0],\n",
    "           [1.3881, 0],\n",
    "           [3.0641, 0],\n",
    "           [7.6275, 1],\n",
    "           [5.3324, 1],\n",
    "           [6.9225, 1],\n",
    "           [8.6754, 1],\n",
    "           [7.6737, 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you have been provided with the coefficient which computed with some mL algorithm for predicting the y value using the x's. xD\n",
    "\n",
    "in other words, we were given $\\beta_0$ and $\\beta_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coef = [-0.806605464, 0.2573316]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking how well our precomputed coefficients works! (using sigmoid logestic function)\n",
    "for row in dataset:\n",
    "    yhat = 1.0 / (1.0 + np.exp(- coef[0] - coef[1] * row[0]))\n",
    "    print(\"yhat {0:.4f}, yhat {1}\".format(yhat, round(yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the real world, we are not gonna have $\\beta_0$ and $\\beta_1$ coefficients and we are use to achieve it. it's a cruel world actually. the crap below is mostly about approaches of finding $\\beta_0$ and $\\beta_1$ coefficients. there is approaches like \"maximum-likelihood estimation\" and \"gradient descent\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning the Logistic Regression Model\n",
    "\n",
    "The coefficients (Beta values b) of the logistic regression algorithm must be estimated from your training data. This is done using [maximum-likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation).\n",
    "\n",
    "Maximum-likelihood estimation is a common learning algorithm used by a variety of machine learning algorithms, although it does make assumptions about the distribution of your data (more on this when we talk about preparing your data).\n",
    "\n",
    "The best coefficients would result in a model that would predict a value very close to 1 (e.g. male) for the default class and a value very close to 0 (e.g. female) for the other class. The intuition for maximum-likelihood for logistic regression is that a search procedure seeks values for the coefficients (Beta values) that minimize the error in the probabilities predicted by the model to those in the data (e.g. probability of 1 if the data is the primary class).\n",
    "\n",
    "We are not going to go into the math of maximum likelihood. It is enough to say that a minimization algorithm is used to optimize the best values for the coefficients for your training data. This is often implemented in practice using efficient numerical optimization algorithm (like the Quasi-newton method).\n",
    "\n",
    "When you are learning logistic, you can implement it yourself from scratch using the much simpler gradient descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning with Stochastic Gradient Descent\n",
    "\n",
    "Logistic Regression uses gradient descent to update the coefficients.\n",
    "\n",
    "Each gradient descent iteration, the coefficients are updated using the equation:\n",
    "\n",
    "$$\\beta=\\beta+\\textrm{learning rate}\\times (y-\\hat{y}) \\times \\hat{y} \\times (1-\\hat{y}) \\times x $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Scikit Learn to Estimate Coefficients by itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where is my money bitch?!\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deviding dataset into two variables\n",
    "X = np.array(dataset)[:, 0:1]\n",
    "y = np.array(dataset)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what X values gonna look like?!\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so as Y values...\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiating the regression module\n",
    "clf_LR = LogisticRegression(C=1.0, penalty='l2', tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the model\n",
    "clf_LR.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and predict Y using X values and the model we have learned before\n",
    "clf_LR.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset2 = [[ 0.2,  0. ],\n",
    "            [ 0.2,  0. ],\n",
    "            [ 0.2,  0. ],\n",
    "            [ 0.2,  0. ],\n",
    "            [ 0.2,  0. ],\n",
    "            [ 0.4,  0. ],\n",
    "            [ 0.3,  0. ],\n",
    "            [ 0.2,  0. ],\n",
    "            [ 0.2,  0. ],\n",
    "            [ 0.1,  0. ],\n",
    "            [ 1.4,  1. ],\n",
    "            [ 1.5,  1. ],\n",
    "            [ 1.5,  1. ],\n",
    "            [ 1.3,  1. ],\n",
    "            [ 1.5,  1. ],\n",
    "            [ 1.3,  1. ],\n",
    "            [ 1.6,  1. ],\n",
    "            [ 1. ,  1. ],\n",
    "            [ 1.3,  1. ],\n",
    "            [ 1.4,  1. ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array(dataset2)[:, 0:1]\n",
    "y = np.array(dataset2)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_LR = LogisticRegression(C=1.0, penalty='l2', tol=0.0001)\n",
    "\n",
    "clf_LR.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_LR.predict(X)\n",
    "clf_LR.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.column_stack((y_pred, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we see, the model predicted the values 100% correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
