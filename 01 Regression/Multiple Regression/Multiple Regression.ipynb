{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Regression\n",
    "\n",
    "Simple Linear Regression:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1X$$\n",
    "\n",
    "Multiple Linear Regression:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ...$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boston_data = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(boston_data.data, columns=boston_data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature matrix\n",
    "X = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# target feature (the feature have gonna model it)\n",
    "# HOUSE PRICE VALUE(for sure!)\n",
    "y = boston_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to add a constant term to allow statsmodel.api to calculate the bias / intercepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_constant = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(X_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sm.OLS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using ordinary least square model\n",
    "model = sm.OLS(y, X_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of statistical tests and information. Mostly for the purpose of statistical analysis.\n",
    "\n",
    "we dont need all of these for data science.\n",
    "\n",
    "Data science focus is on prediction and having models that work on predicting real data. It is not concerned as much with  correct specifications of statistical problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Statistical Outputs:\n",
    "\n",
    "**Dep. Variable**: The dependent variable or target variable\n",
    "\n",
    "**Model**: Highlight the model used to obtain this output. It is OLS here. Ordinary least squares / Linear regression\n",
    "\n",
    "**Method**: The method used to fit the data to the model. Least squares\n",
    "\n",
    "**No. Observations**: The number of observations\n",
    "\n",
    "**DF Residuals**: The degrees of freedom of the residuals. Calculated by taking the number of observations less the number of parameters\n",
    "\n",
    "**DF Model**: The number of estimated parameters in the model. In this case 13. The constant term is not included.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R-squared**: This is the coefficient of determination. Measure of goodness of fit.\n",
    "$$R^2=1-\\frac{SS_{res}}{SS_{tot}}$$\n",
    "\n",
    "> From [wiki](https://en.wikipedia.org/wiki/Coefficient_of_determination),\n",
    "\n",
    "  > The total sum of squares, $SS_{tot}=\\sum_i(y_i-\\bar{y})^2$\n",
    "\n",
    "  > The regression sum of squares (explained sum of squares), $SS_{reg}=\\sum_i(f_i-\\bar{y})^2$\n",
    "\n",
    "  > The sum of squares of residuals (residual sum of squares), $SS_{res}=\\sum_i(y_i-f_i)^2 = \\sum_ie^2_i$\n",
    "\n",
    "**Adj. R-squared**: This is the adjusted R-squared. It is the coefficient of determination adjusted by sample size and the number of parameters used.\n",
    "$$\\bar{R}^2=1-(1-R^2)\\frac{n-1}{n-p-1}$$\n",
    "\n",
    "> $p$ = The total number of explanatory variables not including the constant term\n",
    "\n",
    "> $n$ = The sample size\n",
    "\n",
    "**F-statistic**: A measure that tells you if you model is different from a simple average.\n",
    "\n",
    "**Prob (F-statistic)**: This measures the significance of your F-statistic. Also called p-value of F-statistic. In statistics, p-value equal or lower than 0.05 is considered significant.\n",
    "\n",
    "**AIC**: This is the Akaike Information Criterion. It evaluatess the model based on the model complexity and number of observations. The lower the better. \n",
    "\n",
    "**BIC**: This is the Bayesian Information Criterion. Similar to AIC, except it pushishes models with more parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters Estimates and the Associated Statistical Tests\n",
    "\n",
    "**coef**: The estimated coefficient. Note that this is just a point estimate.\n",
    "\n",
    "**std err**: The standard error of the estimate of the coefficient. Another term for standard deviation\n",
    "\n",
    "**t**: The t-statistic score. \n",
    "\n",
    "**P > |t|**: The p-value. A measure of the probability that the coefficient is different from zero.\n",
    "\n",
    "**[95.0% Conf. Interval]**: The 95% confidence interval of the coefficient. Shown here as [0.025, 0.975], the lower and upper bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Tests\n",
    "\n",
    "**Omnibus D'Angostino's test**: This is a combined statistical test for skewness and kurtosis.\n",
    "\n",
    "**Prob(Omnibus)**: p-value of Omnibus test.\n",
    "\n",
    "**Skewness**: This is a measure of the symmetry of the residuals around the mean. Zero if symmetrical. A positive value indicates a long tail to the right; a negative value a long tail to the left.\n",
    "\n",
    "**Kurtosis**: This is a measure of the shape of the distribution of the residuals. A normal distribution has a zero measure. A negative value points to a flatter than normal distribution; a positive one has a higher peak than normal distribution.\n",
    "\n",
    "**Durbin-Watson**: This is a test for the presence of correlation among the residuals. This is especially important for time series modelling\n",
    "\n",
    "**Jarque-Bera**: This is a combined statistical test of skewness and kurtosis.\n",
    "\n",
    "**Prob (JB)**: p-value of Jarque-Bera.\n",
    "\n",
    "**Cond. No**: This is a test for multicollinearity. > 30 indicates unstable results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# statsmodels.formula.api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "form_lr = smf.ols(formula = 'y ~ CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT', \n",
    "              data=df)\n",
    "mlr = form_lr.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "form_lr = smf.ols(formula = 'y ~ CRIM + ZN + INDUS + CHAS + NOX + RM', \n",
    "              data=df)\n",
    "mlr = form_lr.fit()\n",
    "mlr.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix\n",
    "\n",
    "Useful diagnostic tool to identify collinearity between predictors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "corr_matrix = df.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix[np.abs(corr_matrix) < 0.6] = 0\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='YlGnBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Collinearity with Eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(eigenvalues).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that index 8, eigenvalue of 0.0635, is near to zero or very small compared to the others. Small value represents presence of collinearity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.abs(pd.Series(eigenvectors[:,8])).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that index 9, 8, 2 have very high loading when compared against the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns[2], df.columns[8], df.columns[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the factors that are causing multicollinearity problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revisiting Feature Importance and Extractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check:\n",
    "\n",
    "1. Direction of the coefficient\n",
    "2. Impact of the variable / factor on the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardise Variable to Identify Key Feature(s)\n",
    "\n",
    "In order to perform point 2 properly, one needs to standardise the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(list(zip(model.coef_, df.columns)), columns=['coefficient', 'name']).set_index('name')\n",
    "np.abs(result).sort_values(by='coefficient', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  \n",
    "from sklearn.pipeline import make_pipeline  \n",
    "scaler = StandardScaler()  \n",
    "Stand_coef_linear_reg = make_pipeline(scaler, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stand_coef_linear_reg.fit(X,y)\n",
    "result = pd.DataFrame(list(zip(Stand_coef_linear_reg.steps[1][1].coef_, df.columns)), \n",
    "                      columns=['coefficient', 'name']).set_index('name')\n",
    "np.abs(result).sort_values(by='coefficient', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use $R^2$ to Identify Key Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compare $R^2$ of model against $R^2$ of model without a feature. \n",
    "\n",
    "* A significant change in $R^2$ signify the importance of the feature.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg = smf.ols(formula = 'y ~ CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT', \n",
    "              data=df)\n",
    "benchmark = linear_reg.fit()\n",
    "r2_score(y, benchmark.predict(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "without LSTAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg = smf.ols(formula = 'y ~ CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B', \n",
    "              data=df)\n",
    "lr_without_LSTAT = linear_reg.fit()\n",
    "r2_score(y, lr_without_LSTAT.predict(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "without AGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg = smf.ols(formula = 'y ~ CRIM + ZN + INDUS + CHAS + NOX + RM + DIS + RAD + TAX + PTRATIO + B + LSTAT', \n",
    "              data=df)\n",
    "lr_without_AGE = linear_reg.fit()\n",
    "r2_score(y, lr_without_AGE.predict(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
