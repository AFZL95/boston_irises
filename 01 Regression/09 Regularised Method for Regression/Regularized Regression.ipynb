{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Method for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples:\n",
    "\n",
    "* [Ridge Regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)\n",
    "* [Least Absolute Shrinkage and Selection Operator (LASSO)](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)\n",
    "* [Elastic Net](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html)\n",
    "\n",
    "## Ridge Regression\n",
    "\n",
    "Ridge regression addresses some of the problems of **Ordinary Least Squares** by imposing a penalty on the size of coefficients. The ridge coefficients minimize a penalized residual sum of squares,\n",
    "\n",
    "$$\\min_{w}\\big|\\big|Xw-y\\big|\\big|^2_2+\\alpha\\big|\\big|w\\big|\\big|^2_2$$\n",
    "\n",
    "$\\alpha>=0$ is a complexity parameter that controls the amount of shrinkage: the larger the value of $\\alpha$, the greater the amount of shrinkage and thus the coefficients become more robust to collinearity.\n",
    "\n",
    "Ridge regression is an L2 penalized model. Add the squared sum of the weights to the least-squares cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shows the effect of collinearity in the coefficients of an estimator.\n",
    "\n",
    "Ridge Regression is the estimator used in this example. Each color represents a different feature of the coefficient vector, and this is displayed as a function of the regularization parameter.\n",
    "\n",
    "This example also shows the usefulness of applying Ridge regression to highly ill-conditioned matrices. For such matrices, a slight change in the target variable can cause huge variances in the calculated weights. In such cases, it is useful to set a certain regularization (alpha) to reduce this variation (noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "\n",
    "# X is the 10x10 Hilbert matrix\n",
    "X = 1. / (np.arange(1, 11) + np.arange(0, 10)[:, np.newaxis])\n",
    "y = np.ones(10)\n",
    "\n",
    "# ###########################################################################\n",
    "# Compute paths\n",
    "\n",
    "n_alphas = 200\n",
    "alphas = np.logspace(-10, -2, n_alphas)\n",
    "\n",
    "coefs = []\n",
    "for a in alphas:\n",
    "    ridge = linear_model.Ridge(alpha=a, fit_intercept=False)\n",
    "    ridge.fit(X, y)\n",
    "    coefs.append(ridge.coef_)\n",
    "\n",
    "# ###########################################################################\n",
    "# Display results\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "plt.title('Ridge coefficients as a function of the regularization')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO Regression\n",
    "\n",
    "A linear model that estimates sparse coefficients.\n",
    "\n",
    "Mathematically, it consists of a linear model trained with $\\ell_1$ prior as regularizer. The objective function to minimize is:\n",
    "\n",
    "$$\\min_{w}\\frac{1}{2n_{samples}} \\big|\\big|Xw - y\\big|\\big|_2^2 + \\alpha \\big|\\big|w\\big|\\big|_1$$\n",
    "\n",
    "The lasso estimate thus solves the minimization of the least-squares penalty with $\\alpha \\big|\\big|w\\big|\\big|_1$ added, where $\\alpha$ is a constant and $\\big|\\big|w\\big|\\big|_1$ is the $\\ell_1-norm$ of the parameter vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net\n",
    "\n",
    "A linear regression model trained with L1 and L2 prior as regularizer. \n",
    "\n",
    "This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge. \n",
    "\n",
    "Elastic-net is useful when there are multiple features which are correlated with one another. Lasso is likely to pick one of these at random, while elastic-net is likely to pick both.\n",
    "\n",
    "A practical advantage of trading-off between Lasso and Ridge is it allows Elastic-Net to inherit some of Ridgeâ€™s stability under rotation.\n",
    "\n",
    "The objective function to minimize is in this case\n",
    "\n",
    "$$\\min_{w}{\\frac{1}{2n_{samples}} \\big|\\big|X w - y\\big|\\big|_2 ^ 2 + \\alpha \\rho \\big|\\big|w\\big|\\big|_1 +\n",
    "\\frac{\\alpha(1-\\rho)}{2} \\big|\\big|w\\big|\\big|_2 ^ 2}$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers Impact "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "% matplotlib inline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "rng = np.random.randn(n_samples) * 10\n",
    "y_gen = 0.5 * rng + 2 * np.random.randn(n_samples)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(rng.reshape(-1, 1), y_gen)\n",
    "model_pred = lr.predict(rng.reshape(-1,1))\n",
    "\n",
    "plt.figure(figsize=(10,8));\n",
    "plt.scatter(rng, y_gen);\n",
    "plt.plot(rng, model_pred);\n",
    "print(\"Coefficient Estimate: \", lr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = rng.argmax()\n",
    "y_gen[idx] = 200\n",
    "idx = rng.argmin()\n",
    "y_gen[idx] = -200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8));\n",
    "plt.scatter(rng, y_gen);\n",
    "\n",
    "o_lr = LinearRegression(normalize=True)\n",
    "o_lr.fit(rng.reshape(-1, 1), y_gen)\n",
    "o_model_pred = o_lr.predict(rng.reshape(-1,1))\n",
    "\n",
    "plt.scatter(rng, y_gen);\n",
    "plt.plot(rng, o_model_pred);\n",
    "print(\"Coefficient Estimate: \", o_lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_mod = Ridge(alpha=0.5, normalize=True)\n",
    "ridge_mod.fit(rng.reshape(-1, 1), y_gen)\n",
    "ridge_model_pred = ridge_mod.predict(rng.reshape(-1,1))\n",
    "\n",
    "plt.figure(figsize=(10,8));\n",
    "plt.scatter(rng, y_gen);\n",
    "plt.plot(rng, ridge_model_pred);\n",
    "print(\"Coefficient Estimate: \", ridge_mod.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_mod = Lasso(alpha=0.4, normalize=True)\n",
    "lasso_mod.fit(rng.reshape(-1, 1), y_gen)\n",
    "lasso_model_pred = lasso_mod.predict(rng.reshape(-1,1))\n",
    "\n",
    "plt.figure(figsize=(10,8));\n",
    "plt.scatter(rng, y_gen);\n",
    "plt.plot(rng, lasso_model_pred);\n",
    "print(\"Coefficient Estimate: \", lasso_mod.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_mod = ElasticNet(alpha=0.02, normalize=True)\n",
    "en_mod.fit(rng.reshape(-1, 1), y_gen)\n",
    "en_model_pred = en_mod.predict(rng.reshape(-1,1))\n",
    "\n",
    "plt.figure(figsize=(10,8));\n",
    "plt.scatter(rng, y_gen);\n",
    "plt.plot(rng, en_model_pred);\n",
    "print(\"Coefficient Estimate: \", en_mod.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "[Question in StackExchange](https://stats.stackexchange.com/questions/866/when-should-i-use-lasso-vs-ridge)\n",
    "\n",
    "**When should I use Lasso, Ridge or Elastic Net?**\n",
    "\n",
    "Ridge regression can't zero out coefficients; You either end up including all the coefficients in the model, or none of them. \n",
    "\n",
    "LASSO does both parameter shrinkage and variable selection automatically. \n",
    "\n",
    "If some of your covariates are highly correlated, you may want to look at the Elastic Net instead of the LASSO.\n",
    "\n",
    "# Other References\n",
    "\n",
    "1. [The Lasso Page](http://statweb.stanford.edu/~tibs/lasso.html)\n",
    "\n",
    "2. [A simple explanation of the Lasso and Least Angle Regression](http://statweb.stanford.edu/~tibs/lasso/simple.html)\n",
    "\n",
    "3. [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
